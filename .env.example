# Supabase
# Project URL: https://efyysxkdlugpnurvajhp.supabase.co
# Get your keys from: https://supabase.com/dashboard/project/efyysxkdlugpnurvajhp/settings/api
NEXT_PUBLIC_SUPABASE_URL=key_from_supabase
NEXT_PUBLIC_SUPABASE_ANON_KEY=key_from_supabase
SUPABASE_SERVICE_ROLE_KEY=key_from_supabase

# LLM Provider - OpenAI (recommended)
#OPENAI_API_KEY=get_from_openai
OPENAI_MODEL=gpt-4o-mini

# Embeddings Provider - OpenAI (RECOMMENDED)
# Why OpenAI embeddings? Since you're using OpenAI for LLM, using their embeddings provides:
# - Best integration and consistency
# - Excellent quality (text-embedding-3-small is fast and accurate)
# - Cost-effective ($0.02 per 1M tokens)
# - Same API key works for both LLM and embeddings
# Alternative options: Cohere (good quality), Voyage AI (specialized for RAG)
OPENAI_EMBEDDINGS_MODEL=text-embedding-3-small
# Use the same OPENAI_API_KEY for embeddings (no separate key needed)

# Re-ranking Configuration (Phase 12.1)
# Re-ranking improves search result quality by re-scoring candidates using semantic similarity
# and optionally applying diversity (MMR) to avoid redundant results
# Default: disabled (set to 'true' to enable)
ENABLE_RERANKING=false
# Number of candidates to re-rank (typically 20-50, default: 30)
# Higher values = better quality but slower performance
RERANKING_TOP_K_CANDIDATES=30
# Number of final results after re-ranking (default: 8)
RERANKING_TOP_K_RESULTS=8
# Enable Maximal Marginal Relevance (MMR) for diversity
# MMR balances relevance with diversity to avoid returning similar chunks
# Default: disabled (set to 'true' to enable)
ENABLE_MMR=false
# MMR lambda parameter (0-1): 0 = pure relevance, 1 = pure diversity
# Recommended: 0.5 for balanced approach
MMR_LAMBDA=0.5

# Query Processing Configuration (Phase 12.2)
# Query processing improves retrieval by expanding queries with related terms,
# rewriting queries in multiple ways, and understanding query intent
# Default: disabled (set to 'true' to enable)
ENABLE_QUERY_PROCESSING=false
# Enable query expansion: generates related terms and synonyms using LLM
# Default: disabled (set to 'true' to enable)
ENABLE_QUERY_EXPANSION=false
# Enable query rewriting: generates multiple query variations using LLM
# Default: disabled (set to 'true' to enable)
ENABLE_QUERY_REWRITING=false
# Enable query understanding: parses query intent and extracts entities
# Default: disabled (set to 'true' to enable)
ENABLE_QUERY_UNDERSTANDING=false
# Maximum number of expansion terms to generate (default: 5)
QUERY_MAX_EXPANSIONS=5
# Maximum number of query variations to generate (default: 3)
QUERY_MAX_REWRITES=3

# Semantic Chunking Configuration (Phase 12.3)
# Semantic chunking uses embeddings to identify semantic boundaries and create
# more meaningful chunks that preserve context and improve retrieval quality
# Default: disabled (set to 'true' to enable)
USE_SEMANTIC_CHUNKING=false
# Note: When enabled, semantic chunking will:
# - Use embeddings to find optimal split points based on semantic similarity
# - Adapt chunk sizes based on content density (smaller for technical, larger for narrative)
# - Preserve code blocks, tables, lists, and headers as complete units
# - Fall back to fixed-size chunking if semantic analysis fails
# Warning: Enabling semantic chunking increases embedding API calls during ingestion

# Sentry (optional)
NEXT_PUBLIC_SENTRY_DSN=your_sentry_dsn

# Agent/Orchestrator
DEFAULT_TENANT_ID=default-tenant

# Environment
NODE_ENV=development

